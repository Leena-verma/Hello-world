import pandas as pd
import re

# Load your DataFrame
file_path = 'path_to_your_excel_file.xlsx'
transactions_df = pd.read_excel(file_path)

# Define function to extract keywords/phrases based on hyphen-delimited text
def extract_keywords(description):
    # Define your keywords which are expected to appear in phrases
    keywords = ['training', 'certification', 'exam fee', 'scrum master']  # Extend this list as needed
    pattern = re.compile(r'\b(' + '|'.join(keywords) + r')\b', re.IGNORECASE)

    # Search for patterns that might include these keywords surrounded by hyphens
    matches = re.findall(r'[^-]*-\s*(' + pattern.pattern + r')\s*-[^-]*', description)
    
    # Concatenate matches into a single string separated by semicolons
    return '; '.join(matches)

# Apply the function to the Updated Description column
transactions_df['Vendor Details'] = transactions_df['Updated Description'].apply(extract_keywords)

# Save the updated DataFrame
output_path = 'updated_with_vendor_details.xlsx'
transactions_df.to_excel(output_path, index=False)


















import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from collections import Counter

# Ensure you have the necessary NLTK resources downloaded
nltk.download('punkt')
nltk.download('stopwords')

# Assuming 'df' is your DataFrame and it has a column named 'description'

# Function to preprocess text and extract common keywords
def find_common_keywords(texts, top_n=50):
    words = word_tokenize(' '.join(texts).lower())  # Tokenize and convert to lower case
    filtered_words = [word for word in words if word.isalpha() and word not in stopwords.words('english')]
    freq_dist = FreqDist(filtered_words)
    common_keywords = [word for word, freq in freq_dist.most_common(top_n)]
    return common_keywords

# Extract common keywords from the 'description' column
keywords = find_common_keywords(df['description'], top_n=50)

# Function to find the first matching keyword in the description
def assign_keyword(description):
    for keyword in keywords:
        if keyword in description.lower():  # Check if keyword is in description
            return keyword
    return None  # Return None if no keyword is found

# Apply the function to find keywords for each description
df['found_keyword'] = df['description'].apply(assign_keyword)

# Output the DataFrame to see the result
print(df)

















import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk import pos_tag

# Ensure you have the necessary NLTK resources downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

# Function to preprocess text and extract common noun keywords
def find_common_keywords(texts, top_n=50):
    tokens = word_tokenize(' '.join(texts).lower())  # Tokenize and convert to lower case
    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]
    tagged_tokens = pos_tag(filtered_tokens)  # Tag tokens with part of speech
    nouns = [word for word, tag in tagged_tokens if tag.startswith('NN')]  # Filter nouns
    freq_dist = FreqDist(nouns)
    common_keywords = [word for word, freq in freq_dist.most_common(top_n)]
    return common_keywords

# Assuming 'df' is your DataFrame and it has a column named 'description'
# Extract common keywords from the 'description' column
keywords = find_common_keywords(df['description'], top_n=50)

# Function to find the first matching keyword and the word after it in the description
def assign_keyword_and_next(description):
    words = word_tokenize(description.lower())  # Tokenize and convert to lower case
    tagged_words = pos_tag(words)  # Tag tokens with part of speech
    for i, (word, tag) in enumerate(tagged_words):
        if word in keywords and tag.startswith('NN'):
            # If the noun keyword is found, check if there is another word after it
            if i + 1 < len(tagged_words):
                return word + " " + tagged_words[i + 1][0]  # Return the keyword and the next word
            return word  # If there's no word after the keyword, return just the keyword
    return None  # Return None if no keyword is found

# Apply the function to find keywords and the word after for each description
df['keyword_and_next'] = df['description'].apply(assign_keyword_and_next)

# Output the DataFrame to see the result
print(df)










import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# Ensure you have the necessary NLTK resources downloaded
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Function to find the first matching keyword and the valid next or previous word in the description
def assign_keyword_and_context(description, keywords):
    tokens = word_tokenize(description)  # Tokenize the description
    tagged_tokens = pos_tag(tokens)  # Tag tokens with part of speech
    for i, (word, tag) in enumerate(tagged_tokens):
        if word.lower() in keywords and tag.startswith('NN'):  # Check if the word is a noun keyword
            next_index = i + 1
            prev_index = i - 1
            # Find next valid word (skip spaces and hyphens)
            while next_index < len(tagged_tokens) and (tagged_tokens[next_index][0] in ['-', ' '] or not tagged_tokens[next_index][0].isalpha()):
                next_index += 1
            if next_index < len(tagged_tokens):  # If there's a valid next word
                return (word, word + " " + tagged_tokens[next_index][0])
            elif prev_index >= 0:  # If no valid next word, take the previous word
                return (word, tagged_tokens[prev_index][0] + " " + word)
    return (None, None)  # Return None if no keyword is found

# Extract common keywords from the 'description' column (assuming 'keywords' is already defined)
# Function should be defined here if not already

# Apply the function to find keywords and the context for each description
df['keyword'], df['keyword_and_context'] = zip(*df['description'].apply(lambda desc: assign_keyword_and_context(desc, keywords)))

# Output the DataFrame to see the result
print(df)






import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# Ensure you have the necessary NLTK resources downloaded
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Function to find the first matching keyword and the valid next or previous word in the description
def assign_keyword_and_context(description, keywords):
    tokens = word_tokenize(description)  # Tokenize the description
    tagged_tokens = pos_tag(tokens)  # Tag tokens with part of speech
    for i, (word, tag) in enumerate(tagged_tokens):
        if word.lower() in keywords and tag.startswith('NN'):  # Check if the word is a noun keyword
            # Check for next valid word
            next_index = i + 1
            while next_index < len(tagged_tokens) and (tagged_tokens[next_index][0] in ['-', ' '] or not tagged_tokens[next_index][0].isalpha()):
                next_index += 1
            next_word = tagged_tokens[next_index][0] if next_index < len(tagged_tokens) else None
            
            # Check for previous valid word
            prev_index = i - 1
            while prev_index >= 0 and (tagged_tokens[prev_index][0] in ['-', ' '] or not tagged_tokens[prev_index][0].isalpha()):
                prev_index -= 1
            prev_word = tagged_tokens[prev_index][0] if prev_index >= 0 else None
            
            if next_word:
                return (word, word + " " + next_word)
            elif prev_word:
                return (word, prev_word + " " + word)
            
    return (None, None)  # Return None if no keyword is found

# Assuming 'df' is your DataFrame and it has a column named 'description'
# Extract common keywords from the 'description' column (assuming 'keywords' is already defined)
# Function should be defined here if not already

# Apply the function to find keywords and the context for each description
df['keyword'], df['keyword_and_context'] = zip(*df['description'].apply(lambda desc: assign_keyword_and_context(desc, keywords)))

# Output the DataFrame to see the result
print(df)



import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# Ensure NLTK resources are downloaded
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Create a condition and filter DataFrame
condition = (df['column1'] == 1) & (df['column2'] == 1)
filtered_df = df[condition].copy()

# Apply keyword extraction to filtered data
keywords = find_common_keywords(filtered_df['description'], top_n=50)

# Apply keyword and context extraction to the filtered DataFrame
filtered_df['keyword'], filtered_df['keyword_and_context'] = zip(*filtered_df['description'].apply(lambda desc: assign_keyword_and_context(desc, keywords)))

# Prepare the filtered_df by resetting index to help with the merge
filtered_df.reset_index(inplace=True)

# Merge back to the original DataFrame
result_df = pd.merge(df, filtered_df[['index', 'keyword', 'keyword_and_context']], on='index', how='left')

# Clean up - remove the extra 'index' column
result_df.drop(columns='index', inplace=True)

# Display results
print(result_df)




import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function to preprocess and tokenize text
def tokenize(text):
    tokens = word_tokenize(text)
    words = [word.lower() for word in tokens if word.isalpha()]  # Remove punctuation and numbers
    return words

# Combine all descriptions into a single text
all_descriptions = ' '.join(data['updated_Description'].fillna(''))

# Tokenize the combined text
tokens = tokenize(all_descriptions)

# Remove stopwords
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]

# Calculate frequency distribution
freq_dist = nltk.FreqDist(filtered_tokens)

# Print the 40 most common words
print(freq_dist.most_common(40))

